{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHjbiAr9C_cT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyKrvbKBC1qQ"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install sentencepiece\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgSJCTdmDBZK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "# モデルのディレクトリ\n",
        "# strで指定すると、from_pretrainedでなぜかHuggingFaceを参照したため、PathLikeで指定した。\n",
        "output = Path(\"/content/drive/MyDrive/ColabNotebooks/line_to_gpt2/output/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsXoSWqaDUK7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
        "tokenizer.do_lower_case = True\n",
        "model = AutoModelForCausalLM.from_pretrained(output)\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHCIrQPgDkQS"
      },
      "outputs": [],
      "source": [
        "def generate_reply(inp, num_gen=1):\n",
        "    input_text = \"<s>\" + str(inp) + \"[SEP]\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    out = model.generate(input_ids, do_sample=True, max_length=128, num_return_sequences=num_gen\n",
        "                         top_p=0.95, top_k=50, bad_words_ids=[[1], [5]], no_repeat_ngram_size=3)\n",
        "\n",
        "    print(\">\", \"あなた\")\n",
        "    print(inp)\n",
        "    print(\">\", \"Y君\")\n",
        "    for sent in tokenizer.batch_decode(out):\n",
        "        sent = sent.split('[SEP]</s>')[1]\n",
        "        sent = sent.replace('</s>', '')\n",
        "        sent = sent.replace('<br>', '\\n')\n",
        "        print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdI70dkpDnuk"
      },
      "outputs": [],
      "source": [
        "msg = \"おはよー\"\n",
        "generate_reply(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UTkOENkDugC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Try_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
